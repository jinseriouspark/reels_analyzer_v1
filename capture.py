# capture.py
import os
import subprocess
from typing import List, Dict, Any, Tuple, Optional

import cv2
import numpy as np
from PIL import Image

# ìŒì„± ì¸ì‹
try:
    from faster_whisper import WhisperModel
    _HAS_FWHISPER = True
except Exception:
    _HAS_FWHISPER = False

# OCR
import pytesseract


# ===============================
# ğŸ í”„ë ˆì„ ìº¡ì³
# ===============================
def capture_frames(
    video_path: str,
    interval_sec: float = 2.0,
    out_dir: str = "frames",
    limit: Optional[int] = None,
    resize_width: Optional[int] = None,
    jpeg_quality: int = 85,
) -> List[str]:
    """
    ì¼ì • ê°„ê²©(ì´ˆ)ìœ¼ë¡œ í”„ë ˆì„ ìº¡ì³ (ê²½ê³¼ì‹œê°„ ê¸°ë°˜, float ì§€ì›)
    - interval_sec: 0.5 ê°™ì€ ì†Œìˆ˜ ê°€ëŠ¥
    - limit: ìµœëŒ€ ì €ì¥ í”„ë ˆì„ ìˆ˜(ë©”ëª¨ë¦¬/ë””ìŠ¤í¬ ë³´í˜¸)
    - resize_width: ì €ì¥ ì „ì— ê°€ë¡œ ë¦¬ì‚¬ì´ì¦ˆ(px). Noneì´ë©´ ì›ë³¸ í¬ê¸°.
    """
    os.makedirs(out_dir, exist_ok=True)
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        raise ValueError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

    fps = cap.get(cv2.CAP_PROP_FPS) or 0.0
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)

    saved: List[str] = []
    next_capture_t = 0.0  # ë‹¤ìŒ ìº¡ì³ ëª©í‘œ ì‹œê°„(ì´ˆ)
    idx = 0

    # ì½ìœ¼ë©´ì„œ ê²½ê³¼ì‹œê°„ìœ¼ë¡œ ìƒ˜í”Œ
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # í˜„ì¬ í”„ë ˆì„ì˜ ì‹œê°„(ì´ˆ)
        t = (idx / fps) if fps > 0 else (len(saved) * interval_sec)

        if t + 1e-6 >= next_capture_t:
            # ë¦¬ì‚¬ì´ì¦ˆ(ì„ íƒ)
            if resize_width and frame.shape[1] > resize_width:
                h = int(frame.shape[0] * (resize_width / frame.shape[1]))
                frame = cv2.resize(frame, (resize_width, h), interpolation=cv2.INTER_AREA)

            # ì €ì¥
            fname = os.path.join(out_dir, f"frame_{idx:06d}.jpg")
            cv2.imwrite(fname, frame, [cv2.IMWRITE_JPEG_QUALITY, int(jpeg_quality)])
            saved.append(fname)

            # ë‹¤ìŒ ìº¡ì³ ì‹œê°„ ê°±ì‹ 
            next_capture_t += float(interval_sec)

            # limit ì´ˆê³¼ ë°©ì§€
            if limit and len(saved) >= limit:
                break

        idx += 1

    cap.release()
    return saved


# ===============================
# ğŸµ ì˜¤ë””ì˜¤ ì¶”ì¶œ & ASR
# ===============================
def _check_ffmpeg_available() -> bool:
    try:
        p = subprocess.run(["ffmpeg", "-version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return p.returncode == 0
    except Exception:
        return False

def extract_audio(video_path: str, audio_path: str = "temp_audio.wav") -> str:
    try:
        import imageio_ffmpeg
        ffmpeg_bin = imageio_ffmpeg.get_ffmpeg_exe()  # âœ… ë°”ì´ë„ˆë¦¬ ê²½ë¡œ í™•ë³´
    except Exception:
        ffmpeg_bin = "ffmpeg"  # ë¡œì»¬/í™˜ê²½ì— ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ PATH ì‚¬ìš©

    cmd = [
        ffmpeg_bin, "-y", "-i", video_path,
        "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
        audio_path
    ]
    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if p.returncode != 0:
        raise RuntimeError(f"ffmpeg ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨: {p.stderr.decode(errors='ignore')[:300]}")
    return audio_path

def asr_transcribe(
    video_path: str,
    engine: str = "faster-whisper",
    model_size: str = "small",
    device: str = "cpu",
    compute_type: str = "int8",
    beam_size: int = 5,
    keep_audio: bool = False,
) -> Dict[str, Any]:
    """
    ì˜ìƒì—ì„œ ìŒì„± ì¸ì‹ (ASR)
    - engine: "faster-whisper"ë§Œ ì§€ì›
    - model_size: "tiny"|"base"|"small"|"medium"|"large-v3" ë“±
    - device: "cpu"|"cuda"
    - compute_type: "int8"|"float16"|"default" ë“±
    """
    audio_path = extract_audio(video_path)

    try:
        if engine != "faster-whisper":
            raise ValueError(f"Unsupported ASR engine: {engine}")
        if not _HAS_FWHISPER:
            raise RuntimeError("faster-whisper ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install faster-whisper")

        model = WhisperModel(model_size, device=device, compute_type=compute_type)
        segments, info = model.transcribe(audio_path, beam_size=beam_size)
        texts = []
        seg_list = []
        for seg in segments:
            texts.append(seg.text)
            seg_list.append({"start": seg.start, "end": seg.end, "text": seg.text})

        return {
            "language": getattr(info, "language", ""),
            "text": " ".join(texts).strip(),
            "segments": seg_list,
        }
    finally:
        # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë¦¬
        if not keep_audio and os.path.exists(audio_path):
            try:
                os.remove(audio_path)
            except Exception:
                pass


# ===============================
# ğŸ“ OCR (ìƒ/ì¤‘/í•˜ ì˜ì—­, ì „ì²˜ë¦¬ í¬í•¨)
# ===============================
def _preprocess_for_ocr(img_bgr: np.ndarray) -> np.ndarray:
    """OCR ì „ì²˜ë¦¬: grayâ†’CLAHEâ†’median blurâ†’adaptive thresâ†’morph open"""
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    # ëŒ€ë¹„ í–¥ìƒ
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    gray = clahe.apply(gray)
    # ë…¸ì´ì¦ˆ ê°ì†Œ
    gray = cv2.medianBlur(gray, 3)
    # ê°€ë³€ ì„ê³„
    th = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                               cv2.THRESH_BINARY, 21, 10)
    # ì–‡ì€ ë…¸ì´ì¦ˆ ì œê±°
    kernel = np.ones((2, 2), np.uint8)
    th = cv2.morphologyEx(th, cv2.MORPH_OPEN, kernel, iterations=1)
    return th

def _read_image_any(img_path: str) -> Optional[np.ndarray]:
    try:
        img = cv2.imdecode(np.fromfile(img_path, dtype=np.uint8), cv2.IMREAD_COLOR)
        if img is None:
            img = cv2.imread(img_path)  # fallback
        return img
    except Exception:
        return None

def ocr_images_multi_band(
    image_paths: List[str],
    lang: str = "kor+eng",
    psm: int = 6,   # 6: Assume a single uniform block of text
    oem: int = 3,   # 3: Default, based on what is available
    scale_up: float = 1.5,   # ê°€ë…ì„±ì„ ìœ„í•´ í™•ëŒ€
) -> Dict[str, List[str]]:
    """
    í”„ë ˆì„ ì´ë¯¸ì§€ì—ì„œ OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìƒ/ì¤‘/í•˜ 3ë¶„í• )
    - lang: "kor+eng" ê¶Œì¥
    - psm/oem: tesseract config
    - scale_up: ì „ì²˜ë¦¬ ì´ë¯¸ì§€ í™•ëŒ€ ë¹„ìœ¨
    """
    results = {"top": [], "middle": [], "bottom": []}
    config = f"--oem {int(oem)} --psm {int(psm)}"

    for img_path in image_paths:
        img_bgr = _read_image_any(img_path)
        if img_bgr is None:
            for k in results.keys():
                results[k].append("")
            continue

        h, w = img_bgr.shape[:2]
        bands = {
            "top":    img_bgr[0:int(h/3), :],
            "middle": img_bgr[int(h/3):int(2*h/3), :],
            "bottom": img_bgr[int(2*h/3):, :],
        }

        for band_name, band_bgr in bands.items():
            proc = _preprocess_for_ocr(band_bgr)
            if scale_up and scale_up != 1.0:
                proc = cv2.resize(proc, (int(proc.shape[1]*scale_up), int(proc.shape[0]*scale_up)),
                                  interpolation=cv2.INTER_CUBIC)
            # pytesseractëŠ” RGB/ê·¸ë ˆì´ ë‘˜ ë‹¤ ê°€ëŠ¥í•˜ë‚˜, ì—¬ê¸°ì„  ë‹¨ì¼ ì±„ë„ ì´ë¯¸ì§€ ì „ë‹¬
            try:
                txt = pytesseract.image_to_string(proc, lang=lang, config=config)
            except Exception as e:
                txt = ""
            results[band_name].append((txt or "").strip())

    return results

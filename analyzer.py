"""
ğŸ¤– ë¦´ìŠ¤ ë¶„ì„ ì—”ì§„ (Streamlit-ë…ë¦½í˜•)
- Gemini(OpenAI ì„ íƒì )ë¡œ í”„ë ˆì„ ì´ë¯¸ì§€+ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„
- fast_mode ì§€ì›: í”„ë ˆì„ 1ì¥, ë‹¤ìš´ìŠ¤ì¼€ì¼, í† í° ì¶•ì†Œë¡œ ì†ë„ ê°œì„ 
"""

import os
import io
import re
import json
import base64
from datetime import datetime
from typing import Dict, List, Optional

import cv2
from PIL import Image

# ===== Optional: Gemini / OpenAI =====
_HAS_GEMINI = False
try:
    import google.generativeai as genai
    _HAS_GEMINI = True
except Exception:
    genai = None

try:
    import openai  # í˜„ì¬ëŠ” ë”ë¯¸ ê²½ë¡œë§Œ
    _HAS_OPENAI = True
except Exception:
    _HAS_OPENAI = False


class ReelAnalyzer:
    """ë¦´ìŠ¤(ì§§ì€ ì˜ìƒ) ë¶„ì„ê¸°"""

    def __init__(self):
        self.models: Dict[str, object] = {}
        self._init_models()

    # ---------- ëª¨ë¸ ì´ˆê¸°í™” ----------
    def _init_models(self):
        """API í‚¤ê°€ ìˆëŠ” ëª¨ë¸ë§Œ ì´ˆê¸°í™”"""
        if _HAS_GEMINI and os.getenv("GEMINI_API_KEY"):
            try:
                genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
                # ë¹ ë¥¸ ì‘ë‹µ ìœ„ì£¼ ëª¨ë¸
                self.models["gemini"] = genai.GenerativeModel("gemini-1.5-flash")
            except Exception:
                pass

        # í•„ìš” ì‹œ í™•ì¥
        if _HAS_OPENAI and os.getenv("OPENAI_API_KEY"):
            # openai.api_key = os.getenv("OPENAI_API_KEY")
            self.models["openai"] = "gpt-4o-mini"  # placeholder (ì•„ë˜ _analyze_with_openaiì—ì„œ ë”ë¯¸ ì‘ë‹µ)

    # ---------- í¼ë¸”ë¦­ API ----------
    async def analyze_video(
        self,
        file_path: str,
        teacher_id: str,
        model_name: str = "gemini",
        detailed: bool = True,
        include_suggestions: bool = True,
        pre_context: Optional[dict] = None,
        fast_mode: bool = False,
    ) -> Dict:
        """
        ë¹„ë””ì˜¤ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜
        """
        try:
            # 1) ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ (í”„ë ˆì„ ì¶”ì¶œ/ì••ì¶•)
            video_data = self._process_video(file_path, fast_mode=fast_mode)

            # 2) ì„ ìƒë‹˜ í”„ë¡œí•„(í†¤/ìŠ¤íƒ€ì¼)
            teacher_data = self._get_teacher_profile(teacher_id)

            # 3) ë¶„ì„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±(ASR í…ìŠ¤íŠ¸ ë“± pre_context í¬í•¨)
            prompt = self._create_analysis_prompt(
                video_data=video_data,
                teacher_data=teacher_data,
                detailed=detailed,
                include_suggestions=include_suggestions,
                pre_context=pre_context or {},
            )

            # 4) ëª¨ë¸ í˜¸ì¶œ
            raw = await self._analyze_with_ai(
                prompt=prompt,
                frames=video_data["frames"],
                model_name=model_name,
                fast_mode=fast_mode,
            )

            # 5) ê²°ê³¼ ê°€ê³µ
            result = self._process_analysis_result(
                raw_result=raw,
                video_data=video_data,
                teacher_id=teacher_id,
                model_name=model_name,
            )
            return result

        except Exception as e:
            return {
                "error": True,
                "error_message": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    # ---------- ë¹„ë””ì˜¤ ì²˜ë¦¬ ----------
    def _process_video(self, file_path: str, fast_mode: bool = False) -> Dict:
        """
        OpenCVë¡œ ë¹„ë””ì˜¤ë¥¼ ì—´ê³  í”„ë ˆì„ì„ ì¼ì • ê°„ê²©ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ JPEG base64ë¡œ ë‹´ëŠ”ë‹¤.
        fast_mode: í”„ë ˆì„ 1ì¥, 640px, JPEG 70 / ê¸°ë³¸: í”„ë ˆì„ 3ì¥, 960px, JPEG 85
        """
        cap = cv2.VideoCapture(file_path)
        if not cap.isOpened():
            raise ValueError("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        fps = float(cap.get(cv2.CAP_PROP_FPS) or 0.0)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
        duration = (frame_count / fps) if fps > 0 else 0.0
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)

        frames: List[Dict] = []
        max_frames = 1 if fast_mode else 3          # âœ… ì†ë„ ìš°ì„ 
        step = max(1, int(fps)) if fps > 0 else 1   # ì´ˆë‹¹ 1ì¥
        end = min(frame_count, max_frames * step)

        target_w = 640 if fast_mode else 960        # âœ… ë‹¤ìš´ìŠ¤ì¼€ì¼
        jpeg_q = 70 if fast_mode else 85            # âœ… ìš©ëŸ‰ ì¶•ì†Œ

        for i in range(0, end, step):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if not ret:
                continue

            # ë‹¤ìš´ìŠ¤ì¼€ì¼
            if frame.shape[1] > target_w:
                h = int(frame.shape[0] * (target_w / frame.shape[1]))
                frame = cv2.resize(frame, (target_w, h), interpolation=cv2.INTER_AREA)

            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(rgb)
            buf = io.BytesIO()
            img.save(buf, format="JPEG", quality=jpeg_q)
            b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
            frames.append({
                "timestamp": float(i / fps) if fps > 0 else 0.0,
                "image_base64": b64
            })
            if len(frames) >= max_frames:
                break

        cap.release()

        return {
            "filename": os.path.basename(file_path),
            "metadata": {
                "duration": round(duration, 2),
                "fps": fps,
                "resolution": f"{width}x{height}",
                "is_vertical": bool(height > width),
                "frame_count": frame_count,
            },
            "frames": frames,
            # ì•„ë˜ ë‘ í‚¤ëŠ” appì—ì„œ setdefaultë¡œ í•©ì³ì¤Œ(ì—¬ê¸°ì„  ë¹ˆ ê°’ë§Œ ìœ ì§€)
            "audio_data": {"has_audio": True},
            "text_data": {},
        }

    # ---------- êµì‚¬ í”„ë¡œí•„ ----------
    def _get_teacher_profile(self, teacher_id: str) -> Dict:
        """
        ê°„ë‹¨í•œ í•˜ë“œì½”ë”© í”„ë¡œí•„ (DB ì œê±° ë²„ì „).
        í•„ìš” ì‹œ í™•ì¥/ì™¸ë¶€í™” ê°€ëŠ¥.
        """
        profiles = {
            "teacher_jung": {
                "name": "ì •ì„ ìƒë‹˜",
                "teaching_style": "ì¹œê·¼í•˜ì§€ë§Œ í•µì‹¬ì„ ì½• ì§šì–´ì£¼ëŠ”",
                "tone": "ë”°ëœ»í•˜ê³  ì‹¤ìš©ì ì¸ ë§íˆ¬ë¡œ, ëª…í™•í•œ ê·¼ê±°ì™€ í•¨ê»˜ ì œì•ˆí•©ë‹ˆë‹¤."
            },
            "teacher_kim": {
                "name": "ê¹€ì„ ìƒë‹˜",
                "teaching_style": "ë¶„ì„ì ì´ê³  ë°ì´í„° ì§€í–¥ì ì¸",
                "tone": "ëƒ‰ì² í•˜ê³  ê¹”ë”í•œ ë§íˆ¬ë¡œ, ìˆ˜ì¹˜ì™€ ì‚¬ë¡€ë¥¼ ë“­ë‹ˆë‹¤."
            },
            "teacher_lee": {
                "name": "ì´ì„ ìƒë‹˜",
                "teaching_style": "ìœ ë¨¸ëŸ¬ìŠ¤í•˜ì§€ë§Œ í•  ë§ì€ í•˜ëŠ”",
                "tone": "ê°€ë³ê²Œ ì‹œì‘í•´ë„ ëì€ ë¾°ì¡±í•˜ê²Œ ì •ë¦¬í•©ë‹ˆë‹¤."
            },
            "teacher_park": {
                "name": "ë°•ì„ ìƒë‹˜",
                "teaching_style": "ê°ì„±ì ì´ê³  ìŠ¤í† ë¦¬í…”ë§ ì¤‘ì‹¬ì˜",
                "tone": "ì •ì„œì  ê³µê°ì„ ì „ì œë¡œ, ìŠ¤í† ë¦¬ íë¦„ì„ ê°•ì¡°í•©ë‹ˆë‹¤."
            },
        }
        return profiles.get(teacher_id, {
            "name": "ì„ ìƒë‹˜",
            "teaching_style": "ì¹œê·¼í•œ",
            "tone": "ë¶€ë“œëŸ½ì§€ë§Œ ëª…ë£Œí•œ ë§íˆ¬ë¡œ ì¡°ì–¸í•©ë‹ˆë‹¤."
        })

    # ---------- í”„ë¡¬í”„íŠ¸ ----------
    def _create_analysis_prompt(
        self,
        video_data: Dict,
        teacher_data: Dict,
        detailed: bool,
        include_suggestions: bool,
        pre_context: Dict,
    ) -> str:
        teacher_name = teacher_data.get("name", "ì„ ìƒë‹˜")
        teaching_style = teacher_data.get("teaching_style", "ì¹œê·¼í•œ")
        teacher_tone = teacher_data.get("tone", "ëª…í™•í•˜ê³  ì¹œì ˆí•œ ë§íˆ¬")

        meta = video_data.get("metadata", {})
        frames = video_data.get("frames", [])

        asr_text = (pre_context or {}).get("asr_text", "")
        fast_mode = (pre_context or {}).get("fast_mode", False)

        # ìš”ì²­ JSON ìŠ¤í™(ëª¨ë¸ì´ ê·¸ëŒ€ë¡œ ë‚´ë„ë¡ ê°•ì œ)
        json_spec = """
{
  "scores": {
    "visual_appeal": <1-10 number>,
    "content_structure": <1-10 number>,
    "trend_fit": <1-10 number>,
    "emotional_impact": <1-10 number>,
    "viral_potential": <1-10 number>
  },
  "main_feedback": "<3-5ë¬¸ì¥ í•µì‹¬ í”¼ë“œë°±>",
  "specific_improvements": ["<êµ¬ì²´ì  ê°œì„ 1>", "<êµ¬ì²´ì  ê°œì„ 2>", "<êµ¬ì²´ì  ê°œì„ 3>"],
  "encouragement": "<ê²©ë ¤ í•œ ë¬¸ë‹¨>",
  "next_steps": ["<ë‹¤ìŒì— ì‹œë„1>", "<ë‹¤ìŒì— ì‹œë„2>"]
}
""".strip()

        prompt = f"""
ë‹¹ì‹ ì€ '{teacher_name}'ì…ë‹ˆë‹¤. ë§íˆ¬ëŠ” ë‹¤ìŒì„ ë”°ë¦…ë‹ˆë‹¤: {teacher_tone}
ì§€ë„ ìŠ¤íƒ€ì¼: {teaching_style}

# ì˜ìƒ ë©”íƒ€ë°ì´í„°
- íŒŒì¼ëª…: {video_data.get('filename')}
- ê¸¸ì´(ì´ˆ): {meta.get('duration')}
- FPS: {meta.get('fps')}
- í•´ìƒë„: {meta.get('resolution')}
- ì„¸ë¡œì˜ìƒ: {'ì˜ˆ' if meta.get('is_vertical') else 'ì•„ë‹ˆì˜¤'}
- ìƒ˜í”Œ í”„ë ˆì„ ìˆ˜: {len(frames)}

# ë¶„ì„ ìš”ì²­
ì´ ë¦´ìŠ¤(Short-form) ì˜ìƒì„ í‰ê°€í•˜ê³ , {teacher_name}ì˜ ë§íˆ¬/ìŠ¤íƒ€ì¼ë¡œ í”¼ë“œë°±ì„ ì‘ì„±í•˜ì„¸ìš”.
í‰ê°€ ê¸°ì¤€:
1) ì‹œê°ì  ë§¤ë ¥(Visual Appeal)
2) ì»¨í…ì¸  êµ¬ì„±(Content Structure)
3) íŠ¸ë Œë“œ ì í•©ì„±(Trend Fit)
4) ê°ì •ì  ì„íŒ©íŠ¸(Emotional Impact)
5) ë°”ì´ëŸ´ ê°€ëŠ¥ì„±(Viral Potential)

{('â€» ë¹ ë¥¸ëª¨ë“œ: ê°„ê²°í•˜ê²Œ ìš”ì ë§Œ ì œì‹œí•˜ì„¸ìš”.' if fast_mode else '')}
{('â€» ê°œì„  ì œì•ˆì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.' if include_suggestions else 'â€» ê°œì„  ì œì•ˆì€ ìƒëµí•´ë„ ë©ë‹ˆë‹¤.')}

# ìŒì„± í…ìŠ¤íŠ¸(ìˆë‹¤ë©´ ìš”ì•½ì— ì°¸ê³ ):
{asr_text[:1200] if asr_text else '(ì—†ìŒ)'}

# ì¶œë ¥ í˜•ì‹(ë°˜ë“œì‹œ ìœ íš¨í•œ JSONìœ¼ë¡œë§Œ ì‘ë‹µ):
{json_spec}
""".strip()

        return prompt

    # ---------- ëª¨ë¸ ë””ìŠ¤íŒ¨ì¹˜ ----------
    async def _analyze_with_ai(
        self,
        prompt: str,
        frames: List[Dict],
        model_name: str,
        fast_mode: bool = False,
    ) -> str:
        model_name = (model_name or "").lower()
        if model_name == "gemini":
            return await self._analyze_with_gemini(prompt, frames, fast_mode=fast_mode)
        elif model_name == "openai":
            return await self._analyze_with_openai(prompt, frames, fast_mode=fast_mode)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")

    # ---------- Gemini ----------
    async def _analyze_with_gemini(
        self,
        prompt: str,
        frames: List[Dict],
        fast_mode: bool = False,
    ) -> str:
        if "gemini" not in self.models:
            raise RuntimeError("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GEMINI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        model = self.models["gemini"]
        parts: List[object] = [prompt]

        # ë¹ ë¥¸ëª¨ë“œ: 1ì¥, ì•„ë‹ˆë©´ ìµœëŒ€ 3ì¥
        use_n = 1 if fast_mode else min(3, len(frames))
        for f in frames[:use_n]:
            try:
                img_bytes = base64.b64decode(f["image_base64"])
                # PIL ì´ë¯¸ì§€ë¥¼ ë„˜ê²¨ë„ ë˜ê³  inline_data dictë¡œ ë„˜ê²¨ë„ ë¨
                parts.append({"mime_type": "image/jpeg", "data": img_bytes})
            except Exception:
                continue

        # í† í°/ì˜¨ë„ ì¶•ì†Œ
        cfg = None
        if _HAS_GEMINI:
            cfg = genai.types.GenerationConfig(
                temperature=0.3 if fast_mode else 0.7,
                max_output_tokens=250 if fast_mode else 900,
                response_mime_type="application/json",
            )

        resp = model.generate_content(parts, generation_config=cfg)
        # ëŒ€ë¶€ë¶„ JSON ë¬¸ìì—´ì„ ë°˜í™˜(ì‘ë‹µì´ ë¹„ì–´ìˆì„ ëŒ€ë¹„)
        return (resp.text or "").strip()

    # ---------- OpenAI (ë”ë¯¸) ----------
    async def _analyze_with_openai(
        self,
        prompt: str,
        frames: List[Dict],
        fast_mode: bool = False,
    ) -> str:
        """
        í•„ìš” ì‹œ êµ¬í˜„. í˜„ì¬ëŠ” JSON ë”ë¯¸ë¥¼ ë°˜í™˜í•˜ì—¬ ì „ì²´ í”Œë¡œìš°ë¥¼ ìœ ì§€.
        """
        dummy = {
            "scores": {
                "visual_appeal": 7,
                "content_structure": 7,
                "trend_fit": 6,
                "emotional_impact": 7,
                "viral_potential": 6,
            },
            "main_feedback": "OpenAI ë”ë¯¸ ì‘ë‹µ: ì „ë°˜ì ìœ¼ë¡œ êµ¬ì¡°ê°€ ëª…í™•í•˜ê³  ì‹œê°ì  ì„íŒ©íŠ¸ê°€ ì¤€ìˆ˜í•©ë‹ˆë‹¤. "
                             "ì´ˆë°˜ 2ì´ˆì˜ í›„í‚¹ì„ ë” ê°•í•˜ê²Œ ë§Œë“¤ê³ , ìë§‰ ëŒ€ë¹„ë¥¼ ë†’ì´ë©´ ë„ë‹¬ë¥  í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "specific_improvements": ["ì¸íŠ¸ë¡œ 1.0â†’0.6ì´ˆ", "CTA ìë§‰ ëŒ€ë¹„ 20%â†‘", "BGM ë“œë¡­ìœ¼ë¡œ ì „í™˜ ê°•ì¡°"],
            "encouragement": "ì¢‹ì€ ì‹œë„ì…ë‹ˆë‹¤. ì‘ì€ ê°œì„ ì´ ëˆ„ì ë˜ë©´ ë„ë‹¬Â·ì™„ì‹œìœ¨ì´ ì˜¬ë¼ê°‘ë‹ˆë‹¤!",
            "next_steps": ["í›„í‚¹ 3ì•ˆ A/B í…ŒìŠ¤íŠ¸", "ì”¬ ì „í™˜ ë¦¬ë“¬ ë‹¤ì–‘í™”"],
        }
        return json.dumps(dummy, ensure_ascii=False)

    # ---------- ê²°ê³¼ ê°€ê³µ ----------
    def _process_analysis_result(
        self,
        raw_result: str,
        video_data: Dict,
        teacher_id: str,
        model_name: str,
    ) -> Dict:
        """
        ëª¨ë¸ ì›ë¬¸(raw_result)ì„ JSONìœ¼ë¡œ íŒŒì‹±í•˜ê³ , ì•±ì—ì„œ ì“°ê¸° ì¢‹ì€ í˜•íƒœë¡œ ë³€í™˜
        """
        try:
            data = None

            # 1) ìš°ì„  JSONìœ¼ë¡œ ì§íŒŒì‹±
            try:
                data = json.loads(raw_result)
            except Exception:
                pass

            # 2) ì‹¤íŒ¨í•˜ë©´ ë¬¸ìì—´ ì•ˆì˜ JSON blob ì¶”ì¶œ
            if data is None:
                m = re.search(r'\{[\s\S]*\}', raw_result)
                if m:
                    try:
                        data = json.loads(m.group(0))
                    except Exception:
                        pass

            # 3) ê·¸ë˜ë„ ì‹¤íŒ¨ â†’ ìµœì†Œ êµ¬ì¡° í™•ë³´
            if data is None or not isinstance(data, dict):
                data = {
                    "scores": {
                        "visual_appeal": 7,
                        "content_structure": 7,
                        "trend_fit": 6,
                        "emotional_impact": 7,
                        "viral_potential": 6,
                    },
                    "main_feedback": (raw_result[:400] if isinstance(raw_result, str) else "ë¶„ì„ ê²°ê³¼(ì›ë¬¸) ì—†ìŒ"),
                    "specific_improvements": ["JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì œì•ˆ ì œê³µ"],
                    "encouragement": "ë‹¤ì‹œ ì‹œë„í•´ë³´ë©´ ë” ì •êµí•œ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆì–´ìš”.",
                    "next_steps": ["ì¸íŠ¸ë¡œ í›„í‚¹ ê°•í™”", "CTA ìë§‰ ëŒ€ë¹„ ì¡°ì •"],
                }

            scores = data.get("scores", {})
            overall = self._calculate_overall_score(scores)

            # ìµœì¢… í”¼ë“œë°± í…ìŠ¤íŠ¸(ì„¹ì…˜ í•©ì¹˜ê¸°)
            final_feedback = self._generate_final_feedback(data)

            # ì•±ì—ì„œ ë°”ë¡œ ì“°ë„ë¡ êµ¬ì„±
            result = {
                "analysis_id": f"analysis_{int(datetime.now().timestamp())}",
                "video_id": f"video_{int(datetime.now().timestamp())}",
                "teacher_id": teacher_id,
                "model_name": model_name,
                "scores": scores,  # dict ê·¸ëŒ€ë¡œ (app.pyì˜ normalize_scoresê°€ ì²˜ë¦¬)
                "final_feedback_text": final_feedback,
                "overall_score": overall,
                "specific_improvements": data.get("specific_improvements", []),
                "next_steps": data.get("next_steps", []),
                "timestamp": datetime.now().isoformat(),
                "raw_response": (raw_result or "")[:2000],
                # í‘œì‹œìš© ì›ë¬¸ í•„ë“œ(ì„ íƒ)
                "main_feedback": data.get("main_feedback", ""),
                "encouragement": data.get("encouragement", ""),
            }
            return result

        except Exception as e:
            return {
                "analysis_id": f"error_{int(datetime.now().timestamp())}",
                "error": True,
                "error_message": f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}",
                "raw_response": raw_result,
                "timestamp": datetime.now().isoformat(),
            }

    def _generate_final_feedback(self, data: Dict) -> str:
        """í”¼ë“œë°±/ê°œì„ /ë‹¤ìŒë‹¨ê³„ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°"""
        parts = []
        if data.get("main_feedback"):
            parts.append(data["main_feedback"])

        if data.get("specific_improvements"):
            parts.append("\nğŸ“ êµ¬ì²´ì  ê°œì„ ì‚¬í•­:")
            for i, imp in enumerate(data["specific_improvements"], 1):
                parts.append(f"{i}. {imp}")

        if data.get("next_steps"):
            parts.append("\nğŸš€ ë‹¤ìŒì— ì‹œë„í•´ë³´ì„¸ìš”:")
            for step in data["next_steps"]:
                parts.append(f"â€¢ {step}")

        if data.get("encouragement"):
            parts.append(f"\nğŸ’ª {data['encouragement']}")

        return "\n".join(parts).strip()

    def _calculate_overall_score(self, scores: Dict) -> float:
        """ìˆ«ì ì ìˆ˜ í‰ê· """
        if not scores or not isinstance(scores, dict):
            return 7.0
        vals = []
        for v in scores.values():
            try:
                vals.append(float(v))
            except Exception:
                continue
        if not vals:
            return 7.0
        return round(sum(vals) / len(vals), 1)
